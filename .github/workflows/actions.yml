name: build_parser
on: [push, workflow_dispatch]
jobs:
  build:
    name: build
    runs-on: ubuntu-22.04
    steps:
      - uses: actions/checkout@v3

      - uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/bin/
            ~/.cargo/registry/index/
            ~/.cargo/registry/cache/
            ~/.cargo/git/db/
            target/
          key: cargo-${{ hashFiles('**/Cargo.lock') }}
      - uses: actions/cache@v3
        with:
          path: target/release/baseball-computer
          key: build-${{ github.run_id }}

      - run: cargo build --release

  retrosheet:
    name: retrosheet
    runs-on: ubuntu-latest
    outputs:
      sha: ${{ steps.sha.outputs.sha }}
    steps:
      - id: sha
        run: echo "::set-output name=sha::$(git ls-remote https://github.com/droher/retrosheet-mirror working)"

      - uses: actions/cache@v3
        id: cache
        with:
          path: retrosheet/
          key: retrosheet-${{ steps.sha.outputs.sha }}

      - if: steps.cache.outputs.cache-hit != 'true'
        run: wget https://github.com/droher/retrosheet-mirror/archive/working.zip -O retrosheet.zip
      - if: steps.cache.outputs.cache-hit != 'true'
        run: unzip retrosheet.zip && mv retrosheet-* retrosheet

  simple_files:
    name: Parse simple files
    needs: [retrosheet]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - uses: actions/cache@v3
        with:
          path: retrosheet/
          key: retrosheet-${{ needs.retrosheet.outputs.sha }}

      - uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: pip-${{ hashFiles('**/requirements.txt') }}

      - run: pip install -r bin/requirements.txt
      - run: python bin/simple_files.py

      - run: |
          aws s3 sync --endpoint-url=${{ secrets.R2_ENDPOINT }} \
            retrosheet_simple s3://timeball/misc \
            --exclude '*' --include '*.parquet' --delete

      # TODO: Add back after databank is restored
      # aws s3 sync --endpoint-url=${{ secrets.R2_ENDPOINT }} \
      # baseballdatabank s3://timeball/baseballdatabank \
      # --exclude '*' --include '*.parquet' --delete #}
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET }}
          AWS_DEFAULT_REGION: auto

  pbp:
    name: Create play-by-play
    needs: [build, retrosheet]
    runs-on: ubuntu-22.04
    steps:
      - uses: actions/checkout@v3

      - uses: actions/cache@v3
        with:
          path: retrosheet/
          key: retrosheet-${{ needs.retrosheet.outputs.sha }}

      - uses: actions/cache@v3
        with:
          path: target/release/baseball-computer
          key: build-${{ github.run_id }}
      - run: mkdir csv parquet arrow
      - run: ./target/release/baseball-computer -i retrosheet -o csv

      - uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: pip-${{ hashFiles('**/requirements.txt') }}

      - run: pip install -r bin/requirements.txt
      - run: python bin/parquet.py

      - run: |
          aws s3 sync --endpoint-url=${{ secrets.R2_ENDPOINT }} \
            parquet s3://timeball/event \
            --exclude '*' --include '*.parquet' --delete
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET }}
          AWS_DEFAULT_REGION: auto

  purge_cache:
    name: Purge Cloudflare cache
    needs: [pbp, simple_files]
    runs-on: ubuntu-22.04
    steps:
      # baseball.computer
      - run: |
          curl -X POST "https://api.cloudflare.com/client/v4/zones/${{ secrets.CLOUDFLARE_ZONE }}/purge_cache" \
            -H "Authorization: Bearer ${{ secrets.PURGE_CACHE_TOKEN }}" \
            -H "Content-Type: application/json" \
            --data '{"purge_everything": true}'
